{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header",
            "metadata": {},
            "source": [
                "# Airline Flight Insights - Full Pipeline\n",
                "\n",
                "This notebook provides a complete Graph-RAG pipeline including:\n",
                "1. **Neo4j Database Connection**\n",
                "2. **LLM Setup** (Gemini)\n",
                "3. **Embeddings** - Vector embeddings for semantic search\n",
                "4. **Hybrid Retrieval** - Cypher + Semantic search\n",
                "5. **Question Answering**"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section-1",
            "metadata": {},
            "source": [
                "## 1. Imports and Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "from neo4j import GraphDatabase, Driver\n",
                "from dotenv import load_dotenv, find_dotenv\n",
                "from langchain_groq import ChatGroq\n",
                "from langchain_google_genai import ChatGoogleGenerativeAI\n",
                "from sentence_transformers import SentenceTransformer\n",
                "from typing import List, Dict, Any, Optional, Callable, Tuple\n",
                "import numpy as np\n",
                "import os\n",
                "import json\n",
                "import re"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "env-loader",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "URI: neo4j+s://d9ac65c9.databases.neo4j.io\n",
                        "Groq API key loaded: Yes\n"
                    ]
                }
            ],
            "source": [
                "# Load environment variables\n",
                "load_dotenv(find_dotenv())\n",
                "\n",
                "NEO4J_URI = os.getenv('NEO4J_URI') or os.getenv('URI')\n",
                "NEO4J_USERNAME = os.getenv('NEO4J_USERNAME') or os.getenv('USERNAME')\n",
                "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD') or os.getenv('PASSWORD')\n",
                "groq_api_key = os.getenv('GROQ_API_KEY') or os.getenv('GROQ')\n",
                "gemini_api_key = os.getenv('GOOGLE_API_KEY')\n",
                "\n",
                "print(f\"URI: {NEO4J_URI}\")\n",
                "print(f\"Groq API key loaded: {'Yes' if groq_api_key else 'No'}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "driver-setup",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Connected to Neo4j!\n"
                    ]
                }
            ],
            "source": [
                "# Create Neo4j driver\n",
                "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
                "driver.verify_connectivity()\n",
                "print(\"Connected to Neo4j!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "llm-setup",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Groq LLM loaded! (llama-3.3-70b-versatile)\n",
                        "Gemini LLM loaded! (gemini-2.0-flash-exp)\n"
                    ]
                }
            ],
            "source": [
                "# Groq: FREE, 30+ requests/minute, very fast!\n",
                "llm = ChatGroq(\n",
                "    model=\"llama-3.3-70b-versatile\",\n",
                "    api_key=groq_api_key,\n",
                "    temperature=0\n",
                ")\n",
                "print(\"Groq LLM loaded! (llama-3.3-70b-versatile)\")\n",
                "\n",
                "gemini = ChatGoogleGenerativeAI(\n",
                "    api_key=gemini_api_key,\n",
                "    model=\"gemini-2.0-flash-exp\",\n",
                "    temperature=0\n",
                ")\n",
                "print(\"Gemini LLM loaded! (gemini-2.0-flash-exp)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section-2",
            "metadata": {},
            "source": [
                "## 2. Cypher Queries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "queries-list",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded 20 queries\n"
                    ]
                }
            ],
            "source": [
                "queries = [\n",
                "    # Intent 1: Operational Delay Diagnostics\n",
                "    \"MATCH (j:Journey)-[:ON]->(f:Flight)-[:ARRIVES_AT]->(a:Airport) RETURN a.station_code AS destination, SUM(j.arrival_delay_minutes) AS total_delay ORDER BY total_delay DESC LIMIT $x\",\n",
                "    \"MATCH (j:Journey)-[:ON]->(f:Flight)-[:ARRIVES_AT]->(a:Airport) RETURN a.station_code AS destination, SUM(j.arrival_delay_minutes) AS total_delay ORDER BY total_delay ASC LIMIT $x\",\n",
                "    \"MATCH (j:Journey)-[:ON]->(f:Flight)-[:DEPARTS_FROM]->(a:Airport) RETURN a.station_code AS origin, SUM(j.arrival_delay_minutes) AS total_delay ORDER BY total_delay DESC LIMIT $x\",\n",
                "    \"MATCH (j:Journey)-[:ON]->(f:Flight)-[:DEPARTS_FROM]->(a:Airport) RETURN a.station_code AS origin, SUM(j.arrival_delay_minutes) AS total_delay ORDER BY total_delay ASC LIMIT $x\",\n",
                "    \"MATCH (o:Airport {station_code: $origin_station_code})<-[:DEPARTS_FROM]-(f:Flight)-[:ARRIVES_AT]->(d:Airport), (j:Journey)-[:ON]->(f) WITH o, d, AVG(j.arrival_delay_minutes) AS avg_delay WHERE avg_delay > $x RETURN o.station_code AS origin, d.station_code AS destination, avg_delay\",\n",
                "    \"MATCH (j:Journey {number_of_legs: $x}) RETURN AVG(j.arrival_delay_minutes) AS avg_delay\",\n",
                "    # Intent 2: Service Quality\n",
                "    \"MATCH (o:Airport)<-[:DEPARTS_FROM]-(f:Flight)-[:ARRIVES_AT]->(d:Airport), (j:Journey {passenger_class: $class_name})-[:ON]->(f) WITH o, d, AVG(j.food_satisfaction_score) AS avg_food_score WHERE avg_food_score < $threshold RETURN o.station_code AS origin, d.station_code AS destination, avg_food_score\",\n",
                "    \"MATCH (j:Journey {food_satisfaction_score: 1})-[:ON]->(f:Flight) WHERE j.actual_flown_miles > $x RETURN DISTINCT f.flight_number\",\n",
                "    # Intent 3: Fleet Performance\n",
                "    \"MATCH (j:Journey)-[:ON]->(f:Flight) WHERE j.arrival_delay_minutes > $x RETURN f.fleet_type_description AS aircraft_type, COUNT(j) AS delay_frequency ORDER BY delay_frequency DESC LIMIT 1\",\n",
                "    \"MATCH (j:Journey)-[:ON]->(f:Flight {fleet_type_description: $x}) RETURN AVG(j.food_satisfaction_score) AS avg_food_score\",\n",
                "    \"MATCH (j:Journey)-[:ON]->(f:Flight {fleet_type_description: $x}) RETURN AVG(j.actual_flown_miles) AS avg_miles\",\n",
                "    \"MATCH (j:Journey)-[:ON]->(f:Flight {fleet_type_description: $x}) WITH COUNT(j) AS total_flights, COUNT(CASE WHEN j.arrival_delay_minutes < 0 THEN 1 END) AS early_flights RETURN (TOFLOAT(early_flights) / total_flights) * 100 AS early_arrival_percentage\",\n",
                "    # Intent 3b: Aircraft Performance Aggregation (NEW)\n",
                "    \"MATCH (j:Journey)-[:ON]->(f:Flight) RETURN f.fleet_type_description AS aircraft_type, AVG(j.arrival_delay_minutes) AS avg_delay, COUNT(j) AS flight_count ORDER BY avg_delay ASC LIMIT $x\",\n",
                "    \"MATCH (j:Journey)-[:ON]->(f:Flight) RETURN f.fleet_type_description AS aircraft_type, AVG(j.arrival_delay_minutes) AS avg_delay, COUNT(j) AS flight_count ORDER BY avg_delay DESC LIMIT $x\",\n",
                "    \"MATCH (j:Journey)-[:ON]->(f:Flight) WITH f.fleet_type_description AS aircraft_type, COUNT(j) AS total, COUNT(CASE WHEN j.arrival_delay_minutes <= 0 THEN 1 END) AS on_time RETURN aircraft_type, (toFloat(on_time) / total) * 100 AS on_time_pct, total AS flight_count ORDER BY on_time_pct DESC LIMIT $x\",\n",
                "    # Intent 4: Loyalty\n",
                "    \"MATCH (p:Passenger {loyalty_program_level: $loyalty_program_level})-[:TOOK]->(j:Journey) RETURN AVG(j.arrival_delay_minutes) AS avg_delay\",\n",
                "    \"MATCH (p:Passenger {loyalty_program_level: $loyalty_program_level})-[:TOOK]->(j:Journey) WHERE j.arrival_delay_minutes > $x RETURN p.record_locator AS passenger_id, j.arrival_delay_minutes AS delay\",\n",
                "    # Intent 5: Demographics\n",
                "    \"MATCH (p:Passenger {generation: $generation})-[:TOOK]->(j:Journey)-[:ON]->(f:Flight) WHERE j.actual_flown_miles > $threshold RETURN f.fleet_type_description AS aircraft_type, COUNT(f) AS usage_count ORDER BY usage_count DESC LIMIT 1\",\n",
                "    \"MATCH (p:Passenger {generation: $generation})-[:TOOK]->(j:Journey)-[:ON]->(f:Flight) RETURN f.fleet_type_description AS fleet_type, COUNT(f) AS usage_count ORDER BY usage_count DESC LIMIT 1\",\n",
                "    \"MATCH (p:Passenger {generation: $generation})-[:TOOK]->(j:Journey)-[:ON]->(f:Flight)-[:ARRIVES_AT]->(a:Airport) RETURN a.station_code AS destination, COUNT(p) AS passenger_volume ORDER BY passenger_volume DESC LIMIT $x\"\n",
                "]\n",
                "\n",
                "query_descriptions = [\n",
                "    \"Identify the top ${x} destination stations with the highest accumulated arrival delay minutes.\",\n",
                "    \"Identify the top ${x} destination stations with the lowest accumulated arrival delay minutes.\",\n",
                "    \"Identify the top ${x} origin stations with the highest accumulated arrival delay minutes.\",\n",
                "    \"Identify the top ${x} origin stations with the lowest accumulated arrival delay minutes.\",\n",
                "    \"Find routes from the origin station ${origin_station_code} where the average arrival delay exceeds ${x} minutes.\",\n",
                "    \"Calculate the average arrival delay for flights consisting of exactly ${x} legs.\",\n",
                "    \"Identify routes for the passenger class ${class_name} where the average food satisfaction score is below ${threshold}.\",\n",
                "    \"List the flight numbers for journeys longer than ${x} miles where the food satisfaction score was 1.\",\n",
                "    \"Identify the aircraft type that has the highest frequency of arrival delays greater than ${x} minutes.\",\n",
                "    \"Calculate the average food satisfaction score for passengers flying on the ${x} fleet.\",\n",
                "    \"Calculate the average actual flown miles for the ${x} fleet.\",\n",
                "    \"Calculate the percentage of early arrivals for the ${x} fleet.\",\n",
                "    # NEW: Aircraft performance aggregation\n",
                "    \"List the top ${x} aircraft types with the LOWEST average arrival delay (best on-time performance).\",\n",
                "    \"List the top ${x} aircraft types with the HIGHEST average arrival delay (worst on-time performance).\",\n",
                "    \"List the top ${x} aircraft types by on-time arrival percentage (arrivals with delay <= 0 minutes).\",\n",
                "    # Loyalty\n",
                "    \"Calculate the average arrival delay experienced by passengers with the loyalty level ${loyalty_program_level}.\",\n",
                "    \"Find the record locators for passengers with loyalty level ${loyalty_program_level} who experienced a delay greater than ${x} minutes.\",\n",
                "    # Demographics\n",
                "    \"Identify the most common aircraft type used by the ${generation} generation for journeys exceeding ${threshold} miles.\",\n",
                "    \"Identify the most frequently used fleet type for the ${generation} generation.\",\n",
                "    \"Identify the top ${x} destination stations for the ${generation} generation based on passenger volume.\"\n",
                "]\n",
                "\n",
                "print(f\"Loaded {len(queries)} queries\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section-3",
            "metadata": {},
            "source": [
                "## 3. Query Execution Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "run-query",
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_query(query_index: int, **params) -> list:\n",
                "    \"\"\"Run a query by index with parameters.\"\"\"\n",
                "    if query_index < 0 or query_index >= len(queries):\n",
                "        raise ValueError(f\"Query index {query_index} out of range (0-{len(queries)-1})\")\n",
                "    with driver.session() as session:\n",
                "        result = session.run(queries[query_index], **params)\n",
                "        return [record.data() for record in result]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "kg-schema",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded KG schema:\n",
                        "  - 158 airport codes\n",
                        "  - Generations: ['Boomer', 'Gen X', 'Gen Z', 'Millennial', 'NBK', 'Silent']\n",
                        "  - Loyalty levels: ['NBK', 'global services', 'non-elite', 'premier 1k', 'premier gold', 'premier platinum', 'premier silver']\n",
                        "  - Fleet types: 20 types\n",
                        "  - Passenger classes: ['Economy']\n"
                    ]
                }
            ],
            "source": [
                "# Load KG schema values for better parameter matching\n",
                "def load_kg_schema(driver) -> Dict[str, Any]:\n",
                "    \"\"\"Query the KG to get valid values for each parameter field.\"\"\"\n",
                "    schema = {}\n",
                "    \n",
                "    with driver.session() as session:\n",
                "        # Airport codes\n",
                "        result = session.run('MATCH (a:Airport) RETURN DISTINCT a.station_code AS code ORDER BY code')\n",
                "        schema['airport_codes'] = [r['code'] for r in result]\n",
                "        \n",
                "        # Passenger classes\n",
                "        result = session.run('MATCH (j:Journey) RETURN DISTINCT j.passenger_class AS class ORDER BY class')\n",
                "        schema['passenger_classes'] = [r['class'] for r in result if r['class']]\n",
                "        \n",
                "        # Generations\n",
                "        result = session.run('MATCH (p:Passenger) RETURN DISTINCT p.generation AS gen ORDER BY gen')\n",
                "        schema['generations'] = [r['gen'] for r in result if r['gen']]\n",
                "        \n",
                "        # Loyalty levels\n",
                "        result = session.run('MATCH (p:Passenger) RETURN DISTINCT p.loyalty_program_level AS level ORDER BY level')\n",
                "        schema['loyalty_levels'] = [r['level'] for r in result if r['level']]\n",
                "        \n",
                "        # Fleet types\n",
                "        result = session.run('MATCH (f:Flight) RETURN DISTINCT f.fleet_type_description AS fleet ORDER BY fleet')\n",
                "        schema['fleet_types'] = [r['fleet'] for r in result if r['fleet']]\n",
                "        \n",
                "        # Number of legs\n",
                "        result = session.run('MATCH (j:Journey) RETURN DISTINCT j.number_of_legs AS legs ORDER BY legs')\n",
                "        schema['number_of_legs'] = [r['legs'] for r in result if r['legs']]\n",
                "    \n",
                "    return schema\n",
                "\n",
                "# Load schema on startup\n",
                "kg_schema = load_kg_schema(driver)\n",
                "print(f\"Loaded KG schema:\")\n",
                "print(f\"  - {len(kg_schema['airport_codes'])} airport codes\")\n",
                "print(f\"  - Generations: {kg_schema['generations']}\")\n",
                "print(f\"  - Loyalty levels: {kg_schema['loyalty_levels']}\")\n",
                "print(f\"  - Fleet types: {len(kg_schema['fleet_types'])} types\")\n",
                "print(f\"  - Passenger classes: {kg_schema['passenger_classes']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "get-context",
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_context(prompt: str) -> list:\n",
                "    \"\"\"Use Gemini LLM to identify ALL relevant queries and extract parameters with KG schema awareness.\"\"\"\n",
                "    safe_descriptions = [desc.replace('${', '<').replace('}', '>') for desc in query_descriptions]\n",
                "    query_list = \"\\n\".join([f\"{i}: {desc}\" for i, desc in enumerate(safe_descriptions)])\n",
                "    \n",
                "    # Build comprehensive schema reference\n",
                "    schema_info = (\n",
                "        \"=== DATABASE SCHEMA ===\\n\"\n",
                "        \"The knowledge graph contains these entities and relationships:\\n\"\n",
                "        \"- Airport: station_code (e.g., LAX, JFK, ORD)\\n\"\n",
                "        \"- Flight: flight_number, fleet_type_description\\n\"\n",
                "        \"- Journey: passenger_class, food_satisfaction_score (1-5), arrival_delay_minutes, actual_flown_miles, number_of_legs\\n\"\n",
                "        \"- Passenger: generation, loyalty_program_level\\n\\n\"\n",
                "        \"=== VALID VALUES ===\\n\"\n",
                "        f\"- airport codes: {kg_schema['airport_codes'][:20]}... ({len(kg_schema['airport_codes'])} total)\\n\"\n",
                "        f\"- generation: {kg_schema['generations']}\\n\"\n",
                "        f\"- loyalty_program_level: {kg_schema['loyalty_levels']}\\n\"\n",
                "        f\"- passenger_class: {kg_schema['passenger_classes']}\\n\"\n",
                "        f\"- fleet_type_description: {kg_schema['fleet_types']}\\n\"\n",
                "        f\"- number_of_legs: {kg_schema['number_of_legs']}\\n\"\n",
                "    )\n",
                "    \n",
                "    full_prompt = f\"\"\"You are an expert at analyzing user questions about airline flight data and mapping them to database queries.\n",
                "\n",
                "=== AVAILABLE QUERIES ===\n",
                "{query_list}\n",
                "\n",
                "{schema_info}\n",
                "\n",
                "=== YOUR TASK ===\n",
                "1. Carefully read and understand the user's question\n",
                "2. Review ALL available queries above and understand what each one retrieves\n",
                "3. Identify EVERY query that could help answer the user's question (even partially)\n",
                "4. Extract the correct parameters for each selected query\n",
                "\n",
                "=== PARAMETER RULES ===\n",
                "- x: numeric value for counts/limits (default: 5), delay thresholds (default: 30 minutes), or miles\n",
                "- origin_station_code: must be an exact airport code from the list\n",
                "- generation: must match exactly (e.g., 'Baby Boomer' → 'Boomer', 'millennials' → 'Millennial')\n",
                "- loyalty_program_level: must match exactly (e.g., 'gold member' → 'premier gold')\n",
                "- class_name: must match exactly from passenger_class list\n",
                "- For fleet-related queries (indices 9-11), x must be an EXACT fleet type string\n",
                "\n",
                "=== OUTPUT FORMAT ===\n",
                "Return a JSON array with ALL relevant queries. Each object must have:\n",
                "- query_index: the index number of the query (0-{len(queries)-1})\n",
                "- params: object with parameter names and values\n",
                "\n",
                "Example: [{{\"query_index\": 0, \"params\": {{\"x\": 5}}}}, {{\"query_index\": 15, \"params\": {{\"loyalty_program_level\": \"premier gold\"}}}}]\n",
                "\n",
                "IMPORTANT: \n",
                "- Return ALL queries that are relevant, not just one\n",
                "- Return ONLY the JSON array, no explanation or markdown\n",
                "- If multiple queries can answer different aspects of the question, include them all\n",
                "- If no queries are relevant, return an empty array\n",
                "\n",
                "=== USER QUESTION ===\n",
                "{prompt}\n",
                "\n",
                "JSON:\"\"\"\n",
                "    \n",
                "    response = gemini.invoke(full_prompt)\n",
                "    response_text = response.content.strip()\n",
                "    \n",
                "    # Clean up response - remove markdown and extra whitespace\n",
                "    response_text = response_text.replace('```json', '').replace('```', '').strip()\n",
                "    \n",
                "    print(response_text)\n",
                "\n",
                "    # Find the FIRST valid JSON array (ignore any additional lines)\n",
                "    for line in response_text.split('\\n'):\n",
                "        line = line.strip()\n",
                "        if line.startswith('[') and line.endswith(']'):\n",
                "            try:\n",
                "                return json.loads(line)\n",
                "            except json.JSONDecodeError:\n",
                "                continue\n",
                "\n",
                "    # Fallback: try to find any JSON array in the response\n",
                "    json_match = re.search(r'\\[\\s*\\{[^\\[]*\\}\\s*\\]', response_text, re.DOTALL)\n",
                "    if json_match:\n",
                "        try:\n",
                "            return json.loads(json_match.group())\n",
                "        except json.JSONDecodeError as e:\n",
                "            print(f\"JSON parse error: {e}\")\n",
                "            print(f\"Response was: {response_text[:200]}\")\n",
                "    \n",
                "    return []"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "format-result",
            "metadata": {},
            "outputs": [],
            "source": [
                "def format_query_result(query_index: int, **params) -> str:\n",
                "    \"\"\"Run query and format result as context.\"\"\"\n",
                "    if query_index < 0 or query_index >= len(queries):\n",
                "        return f\"Error: Query index {query_index} out of range.\"\n",
                "    \n",
                "    description = query_descriptions[query_index]\n",
                "    for name, value in params.items():\n",
                "        description = description.replace(f\"${{{name}}}\", str(value))\n",
                "    \n",
                "    try:\n",
                "        results = run_query(query_index, **params)\n",
                "    except Exception as e:\n",
                "        return f'Error for \"{description}\": {e}'\n",
                "    \n",
                "    if not results:\n",
                "        return f'\"{description}\": No data found.'\n",
                "    \n",
                "    formatted = []\n",
                "    for r in results:\n",
                "        parts = [f\"{k}: {v:.2f}\" if isinstance(v, float) else f\"{k}: {v}\" for k, v in r.items()]\n",
                "        formatted.append(\"  - \" + \", \".join(parts))\n",
                "    \n",
                "    return f'\"{description}\":\\n' + \"\\n\".join(formatted)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section-4",
            "metadata": {},
            "source": [
                "## 4. Embeddings Module\n",
                "\n",
                "Vector embeddings for semantic search. Models:\n",
                "- `minilm`: all-MiniLM-L6-v2 (384 dims, fast)\n",
                "- `mpnet`: all-mpnet-base-v2 (768 dims, higher quality)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "embedding-config",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Embedding model configurations\n",
                "EMBEDDING_MODELS = {\n",
                "    \"minilm\": {\"name\": \"all-MiniLM-L6-v2\", \"dimensions\": 384, \"property_name\": \"embedding_minilm\"},\n",
                "    \"mpnet\": {\"name\": \"all-mpnet-base-v2\", \"dimensions\": 768, \"property_name\": \"embedding_mpnet\"}\n",
                "}\n",
                "\n",
                "_model_cache: Dict[str, SentenceTransformer] = {}\n",
                "\n",
                "def get_model(model_key: str) -> SentenceTransformer:\n",
                "    \"\"\"Load and cache an embedding model.\"\"\"\n",
                "    if model_key not in EMBEDDING_MODELS:\n",
                "        raise ValueError(f\"Unknown model: {model_key}. Use 'minilm' or 'mpnet'\")\n",
                "    if model_key not in _model_cache:\n",
                "        print(f\"Loading {EMBEDDING_MODELS[model_key]['name']}...\")\n",
                "        _model_cache[model_key] = SentenceTransformer(EMBEDDING_MODELS[model_key][\"name\"])\n",
                "        print(\"Model loaded!\")\n",
                "    return _model_cache[model_key]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "text-representations",
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_journey_sentences(props: Dict[str, Any]) -> List[str]:\n",
                "    \"\"\"Create individual focused sentences for a Journey.\"\"\"\n",
                "    sentences = {}\n",
                "    \n",
                "    # Sentence 1: Route & Flight info\n",
                "    flight_number = props.get('flight_number', '')\n",
                "    fleet_type = props.get('fleet_type', '')\n",
                "    origin = props.get('origin', '')\n",
                "    destination = props.get('destination', '')\n",
                "    if flight_number and origin and destination:\n",
                "        route_text = f\"Flight {flight_number} from {origin} to {destination}\"\n",
                "        if fleet_type:\n",
                "            route_text += f\" operated by {fleet_type}\"\n",
                "        sentences['route'] = route_text + \".\"\n",
                "    \n",
                "    # Sentence 2: Passenger demographics & loyalty\n",
                "    generation = props.get('generation', '')\n",
                "    loyalty = props.get('loyalty_program_level', '')\n",
                "    if generation or loyalty:\n",
                "        passenger_text = f\"Passenger is a {generation}\" if generation else \"Passenger\"\n",
                "        if loyalty:\n",
                "            passenger_text += f\" with {loyalty} loyalty level\"\n",
                "        sentences['passenger'] = passenger_text + \".\"\n",
                "    \n",
                "    # Sentence 3: Journey experience\n",
                "    passenger_class = props.get('passenger_class', 'Economy')\n",
                "    miles = props.get('actual_flown_miles', 0)\n",
                "    delay = props.get('arrival_delay_minutes', 0)\n",
                "    legs = props.get('number_of_legs', 1)\n",
                "    food_score = props.get('food_satisfaction_score', 3)\n",
                "    delay_text = f\"arrived {abs(delay)} minutes early\" if delay < 0 else \"on time\" if delay == 0 else f\"delayed {delay} minutes\"\n",
                "    food_labels = {1: \"very poor\", 2: \"poor\", 3: \"average\", 4: \"good\", 5: \"excellent\"}\n",
                "    sentences['experience'] = f\"{passenger_class} class, {miles:.0f} miles, {legs} leg(s), {delay_text}, {food_labels.get(food_score, 'average')} food.\"\n",
                "    \n",
                "    return sentences\n",
                "\n",
                "\n",
                "def create_journey_pairwise_texts(props: Dict[str, Any]) -> List[str]:\n",
                "    \"\"\"Create PAIRWISE combinations of journey sentences for better relationship capture.\"\"\"\n",
                "    sentences = create_journey_sentences(props)\n",
                "    pairs = []\n",
                "    \n",
                "    # All pairwise combinations\n",
                "    keys = list(sentences.keys())\n",
                "    for i in range(len(keys)):\n",
                "        for j in range(i + 1, len(keys)):\n",
                "            pair = f\"{sentences[keys[i]]} {sentences[keys[j]]}\"\n",
                "            pairs.append(pair)\n",
                "    \n",
                "    # Also include the full text (all 3 combined)\n",
                "    full_text = \" \".join(sentences.values())\n",
                "    pairs.append(full_text)\n",
                "    \n",
                "    return pairs\n",
                "\n",
                "\n",
                "def create_journey_text(props: Dict[str, Any]) -> str:\n",
                "    \"\"\"Create combined text from all journey sentences.\"\"\"\n",
                "    sentences = create_journey_sentences(props)\n",
                "    return \" \".join(sentences.values())\n",
                "\n",
                "\n",
                "def create_flight_text(props: Dict[str, Any], origin: str = None, destination: str = None) -> str:\n",
                "    \"\"\"Create text representation of a Flight node.\"\"\"\n",
                "    flight_num = props.get('flight_number', 'Unknown')\n",
                "    fleet = props.get('fleet_type_description', 'Unknown')\n",
                "    route = f\" from {origin} to {destination}\" if origin and destination else \"\"\n",
                "    return f\"Flight {flight_num} operated by {fleet}{route}.\"\n",
                "\n",
                "\n",
                "def create_passenger_text(props: Dict[str, Any]) -> str:\n",
                "    \"\"\"Create text representation of a Passenger node.\"\"\"\n",
                "    return f\"A {props.get('generation', 'unknown')} passenger with {props.get('loyalty_program_level', 'unknown')} loyalty.\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "embedding-generation",
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_embeddings(texts: List[str], model_key: str = \"minilm\") -> np.ndarray:\n",
                "    \"\"\"Generate embeddings for a list of texts.\"\"\"\n",
                "    model = get_model(model_key)\n",
                "    return model.encode(texts, show_progress_bar=True, convert_to_numpy=True)\n",
                "\n",
                "\n",
                "def generate_single_embedding(text: str, model_key: str = \"minilm\") -> List[float]:\n",
                "    \"\"\"Generate embedding for a single text.\"\"\"\n",
                "    model = get_model(model_key)\n",
                "    return model.encode(text, convert_to_numpy=True).tolist()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "neo4j-fetch",
            "metadata": {},
            "outputs": [],
            "source": [
                "def fetch_journey_nodes(driver: Driver) -> List[Dict[str, Any]]:\n",
                "    \"\"\"Fetch Journey nodes with ENRICHED data including passenger and flight info.\"\"\"\n",
                "    query = \"\"\"\n",
                "    MATCH (p:Passenger)-[:TOOK]->(j:Journey)-[:ON]->(f:Flight)\n",
                "    OPTIONAL MATCH (f)-[:DEPARTS_FROM]->(o:Airport)\n",
                "    OPTIONAL MATCH (f)-[:ARRIVES_AT]->(d:Airport)\n",
                "    RETURN j.feedback_ID AS feedback_ID,\n",
                "           j.passenger_class AS passenger_class,\n",
                "           j.food_satisfaction_score AS food_satisfaction_score,\n",
                "           j.arrival_delay_minutes AS arrival_delay_minutes,\n",
                "           j.actual_flown_miles AS actual_flown_miles,\n",
                "           j.number_of_legs AS number_of_legs,\n",
                "           p.generation AS generation,\n",
                "           p.loyalty_program_level AS loyalty_program_level,\n",
                "           f.flight_number AS flight_number,\n",
                "           f.fleet_type_description AS fleet_type,\n",
                "           o.station_code AS origin,\n",
                "           d.station_code AS destination\n",
                "    \"\"\"\n",
                "    with driver.session() as session:\n",
                "        result = session.run(query)\n",
                "        return [{\"feedback_ID\": r[\"feedback_ID\"], \"properties\": dict(r)} for r in result]\n",
                "\n",
                "\n",
                "def fetch_flight_nodes(driver: Driver) -> List[Dict[str, Any]]:\n",
                "    \"\"\"Fetch all Flight nodes from Neo4j with route info.\"\"\"\n",
                "    query = \"\"\"\n",
                "    MATCH (f:Flight)\n",
                "    OPTIONAL MATCH (f)-[:DEPARTS_FROM]->(origin:Airport)\n",
                "    OPTIONAL MATCH (f)-[:ARRIVES_AT]->(dest:Airport)\n",
                "    RETURN f.flight_number AS flight_number, f.fleet_type_description AS fleet_type_description,\n",
                "           origin.station_code AS origin, dest.station_code AS destination\n",
                "    \"\"\"\n",
                "    with driver.session() as session:\n",
                "        result = session.run(query)\n",
                "        return [{\n",
                "            \"flight_number\": r[\"flight_number\"],\n",
                "            \"fleet_type_description\": r[\"fleet_type_description\"],\n",
                "            \"properties\": {\"flight_number\": r[\"flight_number\"], \"fleet_type_description\": r[\"fleet_type_description\"]},\n",
                "            \"origin\": r[\"origin\"], \"destination\": r[\"destination\"]\n",
                "        } for r in result]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "vector-index",
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_vector_index(driver: Driver, model_key: str, node_label: str = \"Journey\"):\n",
                "    \"\"\"Create a vector index in Neo4j.\"\"\"\n",
                "    config = EMBEDDING_MODELS[model_key]\n",
                "    index_name = f\"{node_label.lower()}_{config['property_name']}\"\n",
                "    \n",
                "    create_query = f\"\"\"\n",
                "    CREATE VECTOR INDEX {index_name} IF NOT EXISTS\n",
                "    FOR (n:{node_label}) ON n.{config['property_name']}\n",
                "    OPTIONS {{indexConfig: {{\n",
                "        `vector.dimensions`: {config['dimensions']},\n",
                "        `vector.similarity_function`: 'cosine'\n",
                "    }}}}\n",
                "    \"\"\"\n",
                "    with driver.session() as session:\n",
                "        try:\n",
                "            session.run(f\"DROP INDEX {index_name} IF EXISTS\")\n",
                "        except: pass\n",
                "        session.run(create_query)\n",
                "        print(f\"Created index: {index_name}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "store-embeddings",
            "metadata": {},
            "outputs": [],
            "source": [
                "def store_journey_embeddings(driver: Driver, feedback_ids: List[str], embeddings: np.ndarray, \n",
                "                              model_key: str = \"minilm\", batch_size: int = 100):\n",
                "    \"\"\"Store embeddings for Journey nodes.\"\"\"\n",
                "    prop = EMBEDDING_MODELS[model_key][\"property_name\"]\n",
                "    query = f\"UNWIND $batch AS item MATCH (j:Journey {{feedback_ID: item.feedback_ID}}) SET j.{prop} = item.embedding\"\n",
                "    \n",
                "    with driver.session() as session:\n",
                "        for i in range(0, len(feedback_ids), batch_size):\n",
                "            batch = [{\"feedback_ID\": feedback_ids[j], \"embedding\": embeddings[j].tolist()} \n",
                "                     for j in range(i, min(i + batch_size, len(feedback_ids)))]\n",
                "            session.run(query, batch=batch)\n",
                "            print(f\"Stored {min(i + batch_size, len(feedback_ids))}/{len(feedback_ids)}...\")\n",
                "\n",
                "\n",
                "def store_journey_multi_embeddings(driver: Driver, journeys: List[Dict], all_embeddings: Dict[str, np.ndarray],\n",
                "                                    model_key: str = \"minilm\", batch_size: int = 100):\n",
                "    \"\"\"Store multiple embeddings per Journey (route, passenger, experience).\"\"\"\n",
                "    base_prop = EMBEDDING_MODELS[model_key][\"property_name\"]\n",
                "    \n",
                "    for emb_type in ['route', 'passenger', 'experience']:\n",
                "        if emb_type not in all_embeddings:\n",
                "            continue\n",
                "        prop = f\"{base_prop}_{emb_type}\"\n",
                "        query = f\"UNWIND $batch AS item MATCH (j:Journey {{feedback_ID: item.feedback_ID}}) SET j.{prop} = item.embedding\"\n",
                "        embeddings = all_embeddings[emb_type]\n",
                "        \n",
                "        with driver.session() as session:\n",
                "            for i in range(0, len(journeys), batch_size):\n",
                "                batch = [{\"feedback_ID\": journeys[j][\"feedback_ID\"], \"embedding\": embeddings[j].tolist()} \n",
                "                         for j in range(i, min(i + batch_size, len(journeys)))]\n",
                "                session.run(query, batch=batch)\n",
                "        print(f\"Stored {emb_type} embeddings\")\n",
                "\n",
                "\n",
                "def store_flight_embeddings(driver: Driver, flights: List[Dict], embeddings: np.ndarray,\n",
                "                            model_key: str = \"minilm\", batch_size: int = 100):\n",
                "    \"\"\"Store embeddings for Flight nodes.\"\"\"\n",
                "    prop = EMBEDDING_MODELS[model_key][\"property_name\"]\n",
                "    query = f\"UNWIND $batch AS item MATCH (f:Flight {{flight_number: item.flight_number, fleet_type_description: item.fleet_type_description}}) SET f.{prop} = item.embedding\"\n",
                "    \n",
                "    with driver.session() as session:\n",
                "        for i in range(0, len(flights), batch_size):\n",
                "            batch = [{\"flight_number\": flights[j][\"flight_number\"], \n",
                "                      \"fleet_type_description\": flights[j][\"fleet_type_description\"],\n",
                "                      \"embedding\": embeddings[j].tolist()} \n",
                "                     for j in range(i, min(i + batch_size, len(flights)))]\n",
                "            session.run(query, batch=batch)\n",
                "            print(f\"Stored {min(i + batch_size, len(flights))}/{len(flights)}...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "semantic-search",
            "metadata": {},
            "outputs": [],
            "source": [
                "def semantic_search_journeys(driver: Driver, query_text: str, model_key: str = \"minilm\", top_k: int = 5) -> List[Dict]:\n",
                "    \"\"\"Semantic search on Journey nodes - returns enriched data.\"\"\"\n",
                "    query_embedding = generate_single_embedding(query_text, model_key)\n",
                "    index_name = f\"journey_{EMBEDDING_MODELS[model_key]['property_name']}\"\n",
                "    \n",
                "    # Updated query to fetch connected entities\n",
                "    search_query = f\"\"\"\n",
                "    CALL db.index.vector.queryNodes('{index_name}', $top_k, $query_embedding)\n",
                "    YIELD node, score\n",
                "    MATCH (p:Passenger)-[:TOOK]->(node)-[:ON]->(f:Flight)\n",
                "    OPTIONAL MATCH (f)-[:DEPARTS_FROM]->(o:Airport)\n",
                "    OPTIONAL MATCH (f)-[:ARRIVES_AT]->(d:Airport)\n",
                "    RETURN node.feedback_ID AS feedback_ID, \n",
                "           node.passenger_class AS passenger_class,\n",
                "           node.food_satisfaction_score AS food_satisfaction_score,\n",
                "           node.arrival_delay_minutes AS arrival_delay_minutes,\n",
                "           node.actual_flown_miles AS actual_flown_miles,\n",
                "           node.number_of_legs AS number_of_legs,\n",
                "           p.generation AS generation,\n",
                "           p.loyalty_program_level AS loyalty_program_level,\n",
                "           f.flight_number AS flight_number,\n",
                "           f.fleet_type_description AS fleet_type,\n",
                "           o.station_code AS origin,\n",
                "           d.station_code AS destination,\n",
                "           score\n",
                "    ORDER BY score DESC\n",
                "    \"\"\"\n",
                "    with driver.session() as session:\n",
                "        result = session.run(search_query, top_k=top_k, query_embedding=query_embedding)\n",
                "        return [{**dict(r), \"similarity_score\": r[\"score\"]} for r in result]\n",
                "\n",
                "\n",
                "def semantic_search_flights(driver: Driver, query_text: str, model_key: str = \"minilm\", top_k: int = 5) -> List[Dict]:\n",
                "    \"\"\"Semantic search on Flight nodes.\"\"\"\n",
                "    query_embedding = generate_single_embedding(query_text, model_key)\n",
                "    index_name = f\"flight_{EMBEDDING_MODELS[model_key]['property_name']}\"\n",
                "    \n",
                "    search_query = f\"\"\"\n",
                "    CALL db.index.vector.queryNodes('{index_name}', $top_k, $query_embedding)\n",
                "    YIELD node, score\n",
                "    MATCH (node)-[:DEPARTS_FROM]->(origin:Airport)\n",
                "    MATCH (node)-[:ARRIVES_AT]->(dest:Airport)\n",
                "    RETURN node.flight_number AS flight_number, node.fleet_type_description AS fleet_type_description,\n",
                "           origin.station_code AS origin, dest.station_code AS destination, score\n",
                "    ORDER BY score DESC\n",
                "    \"\"\"\n",
                "    with driver.session() as session:\n",
                "        result = session.run(search_query, top_k=top_k, query_embedding=query_embedding)\n",
                "        return [{**dict(r), \"similarity_score\": r[\"score\"]} for r in result]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "embedding-helpers",
            "metadata": {},
            "outputs": [],
            "source": [
                "def format_embedding_results(results: List[Dict], node_type: str = \"Journey\") -> str:\n",
                "    \"\"\"Format embedding search results as context with enriched info.\"\"\"\n",
                "    if not results:\n",
                "        return f\"No similar {node_type} nodes found.\"\n",
                "    \n",
                "    lines = [f\"Found {len(results)} relevant {node_type} records:\"]\n",
                "    for i, r in enumerate(results, 1):\n",
                "        if node_type == \"Journey\":\n",
                "            # Rich journey text with all connected info\n",
                "            text = create_journey_text(r)\n",
                "        else:\n",
                "            text = create_flight_text({\"flight_number\": r.get(\"flight_number\"), \n",
                "                                       \"fleet_type_description\": r.get(\"fleet_type_description\")},\n",
                "                                      r.get(\"origin\"), r.get(\"destination\"))\n",
                "        lines.append(f\"  {i}. (score: {r['similarity_score']:.3f}) {text}\")\n",
                "    return \"\\n\".join(lines)\n",
                "\n",
                "\n",
                "def get_embedding_context(driver: Driver, query: str, model_key: str = \"minilm\", top_k: int = 20) -> str:\n",
                "    \"\"\"Get context from embedding-based semantic search.\"\"\"\n",
                "    contexts = []\n",
                "    try:\n",
                "        contexts.append(format_embedding_results(semantic_search_journeys(driver, query, model_key, top_k), \"Journey\"))\n",
                "    except Exception as e:\n",
                "        contexts.append(f\"Journey search error: {e}\")\n",
                "    try:\n",
                "        contexts.append(format_embedding_results(semantic_search_flights(driver, query, model_key, top_k), \"Flight\"))\n",
                "    except Exception as e:\n",
                "        contexts.append(f\"Flight search error: {e}\")\n",
                "    return \"\\n\\n\".join(contexts)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "generate-all-embeddings",
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_and_store_all_embeddings(driver: Driver, model_key: str = \"minilm\"):\n",
                "    \"\"\"Generate and store embeddings for all Journey and Flight nodes.\"\"\"\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Generating embeddings with {EMBEDDING_MODELS[model_key]['name']}\")\n",
                "    print(f\"{'='*60}\\n\")\n",
                "    \n",
                "    # Journeys\n",
                "    print(\"Fetching Journey nodes...\")\n",
                "    journeys = fetch_journey_nodes(driver)\n",
                "    print(f\"Found {len(journeys)} journeys\")\n",
                "    \n",
                "    if journeys:\n",
                "        texts = [create_journey_text(j[\"properties\"]) for j in journeys]\n",
                "        embeddings = generate_embeddings(texts, model_key)\n",
                "        create_vector_index(driver, model_key, \"Journey\")\n",
                "        store_journey_embeddings(driver, [j[\"feedback_ID\"] for j in journeys], embeddings, model_key)\n",
                "    \n",
                "    # Flights\n",
                "    print(\"\\nFetching Flight nodes...\")\n",
                "    flights = fetch_flight_nodes(driver)\n",
                "    print(f\"Found {len(flights)} flights\")\n",
                "    \n",
                "    if flights:\n",
                "        texts = [create_flight_text(f[\"properties\"], f[\"origin\"], f[\"destination\"]) for f in flights]\n",
                "        embeddings = generate_embeddings(texts, model_key)\n",
                "        create_vector_index(driver, model_key, \"Flight\")\n",
                "        store_flight_embeddings(driver, flights, embeddings, model_key)\n",
                "    \n",
                "    print(f\"\\nEmbedding generation complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section-5",
            "metadata": {},
            "source": [
                "## 5. Hybrid Retrieval\n",
                "\n",
                "Combines Cypher queries with embedding-based semantic search."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "id": "hybrid-context",
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_hybrid_context(driver: Driver, prompt: str, model_key: str = \"minilm\", top_k: int = 20) -> Dict[str, Any]:\n",
                "    \"\"\"Get context from both Cypher queries and embedding search.\"\"\"\n",
                "    results = {'cypher_context': [], 'embedding_context': '', 'combined_context': ''}\n",
                "    \n",
                "    # Cypher context\n",
                "    try:\n",
                "        for cq in get_context(prompt):\n",
                "            results['cypher_context'].append(format_query_result(cq['query_index'], **cq['params']))\n",
                "    except Exception as e:\n",
                "        results['cypher_context'].append(f\"Cypher error: {e}\")\n",
                "    \n",
                "    # Embedding context\n",
                "    try:\n",
                "        results['embedding_context'] = get_embedding_context(driver, prompt, model_key, top_k)\n",
                "    except Exception as e:\n",
                "        results['embedding_context'] = f\"Embedding error: {e}\"\n",
                "    \n",
                "    # Combine\n",
                "    cypher_text = '\\n\\n'.join(results['cypher_context'])\n",
                "    results['combined_context'] = f\"=== STRUCTURED QUERY RESULTS ===\\n{cypher_text}\\n\\n=== SEMANTIC SEARCH RESULTS ===\\n{results['embedding_context']}\"\n",
                "    \n",
                "    return results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "id": "answer-hybrid",
            "metadata": {},
            "outputs": [],
            "source": [
                "def answer_with_hybrid_context(driver: Driver, llm: Any, question: str, model_key: str = \"minilm\", debug = False) -> str:\n",
                "    \"\"\"Answer a question using hybrid retrieval.\"\"\"\n",
                "    context_object = get_hybrid_context(driver, question, model_key)\n",
                "    \n",
                "    if debug: \n",
                "        print(context_object)\n",
                "\n",
                "    context = context_object['combined_context']\n",
                "    prompt = f\"\"\"You are an AI assistant for an airline company analyzing flight data.\n",
                "\n",
                "Based on this context from our knowledge graph, answer the user's question.\n",
                "Only use information from the context. If insufficient, say so.\n",
                "\n",
                "CONTEXT:\n",
                "{context}\n",
                "\n",
                "USER QUESTION: {question}\n",
                "\n",
                "ANSWER:\"\"\"\n",
                "    \n",
                "    return llm.invoke(prompt).content"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "id": "compare-methods",
            "metadata": {},
            "outputs": [],
            "source": [
                "def compare_retrieval_methods(driver: Driver, question: str) -> Dict[str, Any]:\n",
                "    \"\"\"Compare results from different retrieval methods.\"\"\"\n",
                "    results = {'cypher_only': [], 'embedding_minilm': '', 'embedding_mpnet': '', \n",
                "               'hybrid_minilm': None, 'hybrid_mpnet': None}\n",
                "    \n",
                "    # Cypher only\n",
                "    try:\n",
                "        for cq in get_context(question):\n",
                "            results['cypher_only'].append(format_query_result(cq['query_index'], **cq['params']))\n",
                "    except Exception as e:\n",
                "        results['cypher_only'] = [f\"Error: {e}\"]\n",
                "    \n",
                "    # Embeddings\n",
                "    for key in ['minilm', 'mpnet']:\n",
                "        try:\n",
                "            results[f'embedding_{key}'] = get_embedding_context(driver, question, key)\n",
                "        except Exception as e:\n",
                "            results[f'embedding_{key}'] = f\"Error: {e}\"\n",
                "    \n",
                "    # Hybrid\n",
                "    results['hybrid_minilm'] = get_hybrid_context(driver, question, \"minilm\")\n",
                "    results['hybrid_mpnet'] = get_hybrid_context(driver, question, \"mpnet\")\n",
                "    \n",
                "    return results\n",
                "\n",
                "\n",
                "def print_comparison(results: Dict[str, Any]):\n",
                "    \"\"\"Print comparison results.\"\"\"\n",
                "    print(\"=\" * 80 + \"\\nRETRIEVAL METHOD COMPARISON\\n\" + \"=\" * 80)\n",
                "    print(\"\\n--- CYPHER ONLY ---\")\n",
                "    for ctx in results['cypher_only']: print(ctx + \"\\n\")\n",
                "    print(\"\\n--- EMBEDDING (MiniLM) ---\\n\" + results['embedding_minilm'])\n",
                "    print(\"\\n--- EMBEDDING (MPNet) ---\\n\" + results['embedding_mpnet'])\n",
                "    if results['hybrid_minilm']:\n",
                "        print(\"\\n--- HYBRID (MiniLM) ---\\n\" + results['hybrid_minilm']['combined_context'])\n",
                "    if results['hybrid_mpnet']:\n",
                "        print(\"\\n--- HYBRID (MPNet) ---\\n\" + results['hybrid_mpnet']['combined_context'])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section-6",
            "metadata": {},
            "source": [
                "## 6. Interactive Q&A"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "id": "ask-function",
            "metadata": {},
            "outputs": [],
            "source": [
                "def ask(question: str, llm: Any, use_hybrid: bool = True, model_key: str = \"minilm\", debug = False) -> str:\n",
                "    \"\"\"Ask a question using the full pipeline.\"\"\"\n",
                "    print(f\"\\n{'='*60}\\nQ: {question}\\nMode: {'Hybrid' if use_hybrid else 'Cypher Only'}\\n{'='*60}\\n\")\n",
                "    \n",
                "    if use_hybrid:\n",
                "        answer = answer_with_hybrid_context(driver, llm, question, model_key, debug)\n",
                "    else:\n",
                "        context_parts = [format_query_result(cq['query_index'], **cq['params']) for cq in get_context(question)]\n",
                "        print({chr(10).join(context_parts)})\n",
                "        prompt = f\"\"\"You are an AI assistant for an airline analyzing flight data.\n",
                "Answer using only this context:\n",
                "\n",
                "{chr(10).join(context_parts)}\n",
                "\n",
                "Question: {question}\n",
                "\n",
                "Answer:\"\"\"\n",
                "        answer = llm.invoke(prompt).content\n",
                "    \n",
                "    print(f\"ANSWER:\\n{'-'*40}\\n{answer}\\n\")\n",
                "    return answer"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section-7-examples",
            "metadata": {},
            "source": [
                "## 7. Usage Examples"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "id": "example-generate",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Uncomment the line above to generate embeddings.\n"
                    ]
                }
            ],
            "source": [
                "# Generate embeddings (run once)\n",
                "# generate_and_store_all_embeddings(driver, \"minilm\")\n",
                "\n",
                "print(\"Uncomment the line above to generate embeddings.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "example-questions",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "Q: How do Millennials travel compared to Baby Boomers?\n",
                        "Mode: Hybrid\n",
                        "============================================================\n",
                        "\n",
                        "[\n",
                        "  {\"query_index\": 17, \"params\": {\"generation\": \"Millennial\", \"threshold\": 1000}},\n",
                        "  {\"query_index\": 18, \"params\": {\"generation\": \"Millennial\"}},\n",
                        "  {\"query_index\": 19, \"params\": {\"x\": 5, \"generation\": \"Millennial\"}},\n",
                        "  {\"query_index\": 17, \"params\": {\"generation\": \"Boomer\", \"threshold\": 1000}},\n",
                        "  {\"query_index\": 18, \"params\": {\"generation\": \"Boomer\"}},\n",
                        "  {\"query_index\": 19, \"params\": {\"x\": 5, \"generation\": \"Boomer\"}}\n",
                        "]\n",
                        "ANSWER:\n",
                        "----------------------------------------\n",
                        "Based on the provided context, here's a comparison of how Millennials travel compared to Baby Boomers:\n",
                        "\n",
                        "1. **Aircraft Type**: Both Millennials and Baby Boomers most commonly use the B737-900 aircraft type for journeys exceeding 1000 miles, with usage counts of 50 and 154, respectively.\n",
                        "\n",
                        "2. **Fleet Type**: The most frequently used fleet type for both generations is also the B737-900, with usage counts of 72 for Millennials and 212 for Baby Boomers.\n",
                        "\n",
                        "3. **Destination Stations**: The top 5 destination stations differ slightly between the two generations. For Millennials, the top destinations are IAX, EWX, DEX, ORX, and SFX, with passenger volumes of 75, 69, 43, 42, and 27, respectively. For Baby Boomers, the top destinations are EWX, IAX, DEX, ORX, and SFX, with passenger volumes of 269, 182, 146, 137, and 126, respectively.\n",
                        "\n",
                        "It appears that while both generations have some similarities in their travel preferences, Baby Boomers tend to travel more frequently and in larger numbers to certain destinations. However, more data would be needed to make a comprehensive comparison of their travel habits.\n",
                        "\n",
                        "Uncomment an example to try the pipeline!\n"
                    ]
                }
            ],
            "source": [
                "# Example questions:\n",
                "# ask(\"What are the top 5 airports with the most delays?\", llm, debug=False)\n",
                "# ask(\"How do Millennials travel compared to Baby Boomers?\", llm, debug=False)\n",
                "# ask(\"Which aircraft type has the best on-time performance?\")\n",
                "# ask(\"What is the flight number of the journey that departs from LAX and arrives at IAX and has generation 'Millennials'?\")\n",
                "# ask(\"What are the different loyalty program levels for a journey that has flight number 2, mention all of them\")\n",
                "\n",
                "print(\"Uncomment an example to try the pipeline!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f0b2a0c8",
            "metadata": {},
            "source": [
                "## 8. LLMs part"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "id": "e8a26fef",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_core.prompts import ChatPromptTemplate\n",
                "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
                "import time\n",
                "import pandas as pd"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "id": "4338cee0",
            "metadata": {},
            "outputs": [],
            "source": [
                "hf_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "id": "ec620b17",
            "metadata": {},
            "outputs": [],
            "source": [
                "def setup_llm_models():\n",
                "    \"\"\"\n",
                "    Setup multiple LLM models from different providers.\n",
                "    \"\"\"\n",
                "\n",
                "    base_models = {\n",
                "        \"Mistral\": HuggingFaceEndpoint(\n",
                "            repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
                "            huggingfacehub_api_token=hf_token,\n",
                "        ),\n",
                "        \"Llama3\": HuggingFaceEndpoint(\n",
                "            repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
                "            huggingfacehub_api_token=hf_token,\n",
                "        ),\n",
                "        \"Gemma\": HuggingFaceEndpoint(\n",
                "            repo_id=\"google/gemma-2-2b-it\",\n",
                "            huggingfacehub_api_token=hf_token,\n",
                "        )\n",
                "    }\n",
                "    models = {name: ChatHuggingFace(llm=model) for name, model in base_models.items()}\n",
                "    \n",
                "    return models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "id": "8e8bd189",
            "metadata": {},
            "outputs": [],
            "source": [
                "\"\"\"\n",
                "LLM Evaluator Module - Extended with Qualitative Comparison\n",
                "\n",
                "This module provides the LLMEvaluator class for comparing multiple LLM models\n",
                "with both quantitative and qualitative metrics.\n",
                "\n",
                "Usage:\n",
                "    from llm_evaluator import LLMEvaluator\n",
                "    \n",
                "    evaluator = LLMEvaluator(models)\n",
                "    evaluator.evaluate_all_models(questions)\n",
                "    evaluator.evaluate_qualitatively_from_results(groq_llm)\n",
                "    print(evaluator.get_qualitative_summary())\n",
                "\"\"\"\n",
                "\n",
                "class LLMEvaluator:\n",
                "    \"\"\"Comprehensive LLM evaluation system with quantitative and qualitative metrics\"\"\"\n",
                "    \n",
                "    def __init__(self, models: Dict, ask_function=None):\n",
                "        \"\"\"\n",
                "        Initialize the evaluator.\n",
                "        \n",
                "        Args:\n",
                "            models: Dict mapping model names to LLM instances\n",
                "            ask_function: Function to use for asking questions (signature: ask(question, llm) -> str)\n",
                "        \"\"\"\n",
                "        self.models = models\n",
                "        self.results = []\n",
                "        self.qualitative_results = []\n",
                "        self.ask = ask_function\n",
                "    \n",
                "    def set_ask_function(self, ask_function):\n",
                "        \"\"\"Set the ask function to use for querying models.\"\"\"\n",
                "        self.ask = ask_function\n",
                "    \n",
                "    def run_single_test(self, model_name: str, model, question: str) -> Dict:\n",
                "        \"\"\"Run a single test case for one model\"\"\"\n",
                "        if self.ask is None:\n",
                "            raise ValueError(\"ask_function not set. Call set_ask_function() first.\")\n",
                "        \n",
                "        start_time = time.time()\n",
                "        try:\n",
                "            response = self.ask(question, model)\n",
                "            elapsed_time = time.time() - start_time\n",
                "            \n",
                "            return {\n",
                "                \"model\": model_name,\n",
                "                \"question\": question,\n",
                "                \"response\": response,\n",
                "                \"time\": elapsed_time,\n",
                "                \"success\": True,\n",
                "                \"error\": None,\n",
                "                \"tokens_estimate\": len(response.split())  # Rough estimate\n",
                "            }\n",
                "        except Exception as e:\n",
                "            elapsed_time = time.time() - start_time\n",
                "            return {\n",
                "                \"model\": model_name,\n",
                "                \"question\": question,\n",
                "                \"response\": None,\n",
                "                \"time\": elapsed_time,\n",
                "                \"success\": False,\n",
                "                \"error\": str(e),\n",
                "                \"tokens_estimate\": 0\n",
                "            }\n",
                "    \n",
                "    def evaluate_all_models(self, questions: List[str]):\n",
                "        \"\"\"Run all test cases for all models\"\"\"\n",
                "        print(\"Starting LLM Evaluation...\\n\")\n",
                "        \n",
                "        self.results = []\n",
                "\n",
                "        for i, question in enumerate(questions, 1):\n",
                "            print(f\"Test Case {i}/{len(questions)}: {question}\")\n",
                "            \n",
                "            for model_name, model in self.models.items():\n",
                "                print(f\"  Testing {model_name}...\", end=\" \")\n",
                "                result = self.run_single_test(model_name, model, question)\n",
                "                self.results.append(result)\n",
                "                \n",
                "                if result[\"success\"]:\n",
                "                    print(f\"✓ ({result['time']:.2f}s)\")\n",
                "                else:\n",
                "                    print(f\"✗ Error: {result['error'][:50]}\")\n",
                "            \n",
                "            print()\n",
                "        \n",
                "        return self.results\n",
                "    \n",
                "    def get_quantitative_metrics(self) -> pd.DataFrame:\n",
                "        \"\"\"Calculate quantitative metrics\"\"\"\n",
                "        df = pd.DataFrame(self.results)\n",
                "        \n",
                "        metrics = df.groupby('model').agg({\n",
                "            'time': ['mean', 'std', 'min', 'max'],\n",
                "            'success': 'mean',\n",
                "            'tokens_estimate': 'mean'\n",
                "        }).round(3)\n",
                "        \n",
                "        metrics.columns = ['Avg Time (s)', 'Std Time', 'Min Time', 'Max Time', \n",
                "                          'Success Rate', 'Avg Tokens']\n",
                "        \n",
                "        return metrics\n",
                "    \n",
                "    def evaluate_qualitatively_from_results(self, evaluator_llm: Any) -> List[Dict[str, Any]]:\n",
                "        \"\"\"\n",
                "        Qualitatively evaluate all stored results using Groq LLM as the judge.\n",
                "        Uses self.results from evaluate_all_models() to get questions and responses.\n",
                "        \n",
                "        Evaluates on: Relevance, Completeness, Naturalness (1-5 scale each)\n",
                "        \n",
                "        Args:\n",
                "            evaluator_llm: Groq LLM instance to evaluate the responses\n",
                "        \n",
                "        Returns:\n",
                "            List of evaluation results for each question\n",
                "        \"\"\"\n",
                "        if not self.results:\n",
                "            print(\"No results available. Run evaluate_all_models() first.\")\n",
                "            return []\n",
                "        \n",
                "        # Group results by question\n",
                "        questions_map = {}\n",
                "        for r in self.results:\n",
                "            q = r['question']\n",
                "            if q not in questions_map:\n",
                "                questions_map[q] = {}\n",
                "            if r['success'] and r['response']:\n",
                "                questions_map[q][r['model']] = r['response']\n",
                "        \n",
                "        self.qualitative_results = []\n",
                "        \n",
                "        for i, (question, responses) in enumerate(questions_map.items(), 1):\n",
                "            print(f\"\\nEvaluating question {i}/{len(questions_map)}: {question[:50]}...\")\n",
                "            \n",
                "            if len(responses) < 2:\n",
                "                print(\"  Skipping - need at least 2 successful responses\")\n",
                "                continue\n",
                "            \n",
                "            # Format responses for evaluation\n",
                "            responses_text = \"\\n\\n\".join([\n",
                "                f\"### MODEL: {name}\\n{response}\" \n",
                "                for name, response in responses.items()\n",
                "            ])\n",
                "            \n",
                "            # Groq evaluation prompt (without correctness per user request)\n",
                "            eval_prompt = f\"\"\"You are an expert evaluator comparing AI model responses for an airline data assistant.\n",
                "\n",
                "QUESTION: {question}\n",
                "\n",
                "RESPONSES TO EVALUATE:\n",
                "{responses_text}\n",
                "\n",
                "Evaluate each response on these criteria (1-5 scale):\n",
                "1. **Relevance**: Does it directly answer the question?\n",
                "2. **Completeness**: Is it thorough without being verbose?\n",
                "3. **Naturalness**: Is it well-written and easy to understand?\n",
                "\n",
                "Provide your evaluation as JSON:\n",
                "{{\n",
                "    \"evaluations\": {{\n",
                "        \"<model_name>\": {{\n",
                "            \"relevance\": <1-5>,\n",
                "            \"completeness\": <1-5>,\n",
                "            \"naturalness\": <1-5>,\n",
                "            \"total\": <sum out of 15>,\n",
                "            \"reasoning\": \"<brief explanation>\"\n",
                "        }}\n",
                "    }},\n",
                "    \"winner\": \"<model_name>\",\n",
                "    \"summary\": \"<one sentence comparison>\"\n",
                "}}\n",
                "\n",
                "Return ONLY valid JSON, no other text.\"\"\"\n",
                "            \n",
                "            try:\n",
                "                evaluation_response = evaluator_llm.invoke(eval_prompt).content\n",
                "                \n",
                "                # Parse evaluation\n",
                "                json_match = re.search(r'\\{[\\s\\S]*\\}', evaluation_response)\n",
                "                if json_match:\n",
                "                    evaluation = json.loads(json_match.group())\n",
                "                else:\n",
                "                    evaluation = {\"raw_response\": evaluation_response, \"parse_error\": True}\n",
                "            except Exception as e:\n",
                "                evaluation = {\"error\": str(e), \"parse_error\": True}\n",
                "            \n",
                "            result = {\n",
                "                \"question\": question,\n",
                "                \"responses\": responses,\n",
                "                \"evaluation\": evaluation\n",
                "            }\n",
                "            self.qualitative_results.append(result)\n",
                "            \n",
                "            # Print winner for this question\n",
                "            if 'parse_error' not in evaluation:\n",
                "                print(f\"  Winner: {evaluation.get('winner', 'N/A')}\")\n",
                "        \n",
                "        return self.qualitative_results\n",
                "    \n",
                "    def get_qualitative_summary(self) -> pd.DataFrame:\n",
                "        \"\"\"\n",
                "        Get aggregated qualitative metrics across all evaluated questions.\n",
                "        \n",
                "        Returns:\n",
                "            DataFrame with average scores per model\n",
                "        \"\"\"\n",
                "        if not self.qualitative_results:\n",
                "            print(\"No qualitative results. Run evaluate_qualitatively_from_results() first.\")\n",
                "            return pd.DataFrame()\n",
                "        \n",
                "        # Collect scores per model\n",
                "        model_scores = {}\n",
                "        model_wins = {}\n",
                "        \n",
                "        for result in self.qualitative_results:\n",
                "            eval_data = result.get('evaluation', {})\n",
                "            if 'parse_error' in eval_data:\n",
                "                continue\n",
                "            \n",
                "            # Track winner\n",
                "            winner = eval_data.get('winner')\n",
                "            if winner:\n",
                "                model_wins[winner] = model_wins.get(winner, 0) + 1\n",
                "            \n",
                "            # Collect scores\n",
                "            for model, scores in eval_data.get('evaluations', {}).items():\n",
                "                if model not in model_scores:\n",
                "                    model_scores[model] = {'relevance': [], 'completeness': [], 'naturalness': [], 'total': []}\n",
                "                \n",
                "                for metric in ['relevance', 'completeness', 'naturalness', 'total']:\n",
                "                    if metric in scores and isinstance(scores[metric], (int, float)):\n",
                "                        model_scores[model][metric].append(scores[metric])\n",
                "        \n",
                "        # Calculate averages\n",
                "        summary_data = []\n",
                "        for model, scores in model_scores.items():\n",
                "            avg_scores = {\n",
                "                'Model': model,\n",
                "                'Avg Relevance': np.mean(scores['relevance']) if scores['relevance'] else 0,\n",
                "                'Avg Completeness': np.mean(scores['completeness']) if scores['completeness'] else 0,\n",
                "                'Avg Naturalness': np.mean(scores['naturalness']) if scores['naturalness'] else 0,\n",
                "                'Avg Total': np.mean(scores['total']) if scores['total'] else 0,\n",
                "                'Wins': model_wins.get(model, 0)\n",
                "            }\n",
                "            summary_data.append(avg_scores)\n",
                "        \n",
                "        df = pd.DataFrame(summary_data)\n",
                "        if not df.empty:\n",
                "            df = df.sort_values('Avg Total', ascending=False).reset_index(drop=True)\n",
                "            df = df.round(2)\n",
                "        return df\n",
                "    \n",
                "    def print_qualitative_results(self):\n",
                "        \"\"\"Pretty print all qualitative evaluation results.\"\"\"\n",
                "        if not self.qualitative_results:\n",
                "            print(\"No qualitative results. Run evaluate_qualitatively_from_results() first.\")\n",
                "            return\n",
                "        \n",
                "        for result in self.qualitative_results:\n",
                "            print(f\"\\n{'='*70}\")\n",
                "            print(f\"QUESTION: {result['question']}\")\n",
                "            print('='*70)\n",
                "            \n",
                "            print(\"\\n📝 RESPONSES:\")\n",
                "            for model, response in result['responses'].items():\n",
                "                print(f\"\\n--- {model} ---\")\n",
                "                print(response[:500] + \"...\" if len(response) > 500 else response)\n",
                "            \n",
                "            print(f\"\\n{'='*70}\")\n",
                "            print(\"📊 EVALUATION:\")\n",
                "            print('='*70)\n",
                "            \n",
                "            eval_data = result['evaluation']\n",
                "            if 'parse_error' not in eval_data:\n",
                "                for model, scores in eval_data.get('evaluations', {}).items():\n",
                "                    print(f\"\\n{model}:\")\n",
                "                    print(f\"  Relevance: {scores.get('relevance', 'N/A')}/5\")\n",
                "                    print(f\"  Completeness: {scores.get('completeness', 'N/A')}/5\")\n",
                "                    print(f\"  Naturalness: {scores.get('naturalness', 'N/A')}/5\")\n",
                "                    print(f\"  TOTAL: {scores.get('total', 'N/A')}/15\")\n",
                "                    print(f\"  Reasoning: {scores.get('reasoning', 'N/A')}\")\n",
                "                \n",
                "                print(f\"\\n🏆 WINNER: {eval_data.get('winner', 'N/A')}\")\n",
                "                print(f\"📋 Summary: {eval_data.get('summary', 'N/A')}\")\n",
                "            else:\n",
                "                print(eval_data.get('raw_response', eval_data.get('error', 'No evaluation available')))\n",
                "    \n",
                "    def export_results(self, filename: str = \"llm_comparison_results.json\"):\n",
                "        \"\"\"Export detailed results to JSON\"\"\"\n",
                "        export_data = {\n",
                "            'quantitative_results': self.results,\n",
                "            'qualitative_results': self.qualitative_results\n",
                "        }\n",
                "        with open(filename, 'w') as f:\n",
                "            json.dump(export_data, f, indent=2)\n",
                "        print(f\"Results exported to {filename}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "id": "8e372df1",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Selected models: ['Mistral', 'Llama3', 'Gemma']\n",
                        "\n",
                        "Starting LLM Evaluation...\n",
                        "\n",
                        "Test Case 1/5: What are the top 5 airports with the most delays?\n",
                        "  Testing Mistral... \n",
                        "============================================================\n",
                        "Q: What are the top 5 airports with the most delays?\n",
                        "Mode: Hybrid\n",
                        "============================================================\n",
                        "\n",
                        "[\n",
                        "  {\"query_index\": 0, \"params\": {\"x\": 5}},\n",
                        "  {\"query_index\": 2, \"params\": {\"x\": 5}}\n",
                        "]\n",
                        "ANSWER:\n",
                        "----------------------------------------\n",
                        " Based on the context provided, the top 5 destinations with the highest accumulated arrival delay minutes are CDX, JAX, SIX, FRX, and MUX. Similarly, the top 5 origin stations with the highest accumulated arrival delay minutes are DEX, EWX, MUX, SFX, and RSX. However, the user question asked for the top 5 airports with the most delays, which is not directly answered in the context.\n",
                        "\n",
                        "✓ (6.80s)\n",
                        "  Testing Llama3... \n",
                        "============================================================\n",
                        "Q: What are the top 5 airports with the most delays?\n",
                        "Mode: Hybrid\n",
                        "============================================================\n",
                        "\n",
                        "[\n",
                        "  {\"query_index\": 0, \"params\": {\"x\": 5}},\n",
                        "  {\"query_index\": 2, \"params\": {\"x\": 5}}\n",
                        "]\n",
                        "ANSWER:\n",
                        "----------------------------------------\n",
                        "I am unable to determine the top 5 airports with the most delays from the given context. The provided data does not contain airport-specific delay information. It includes destination and origin stations but does not give the accumulated delay minutes for each airport.\n",
                        "\n",
                        "✓ (3.10s)\n",
                        "  Testing Gemma... \n",
                        "============================================================\n",
                        "Q: What are the top 5 airports with the most delays?\n",
                        "Mode: Hybrid\n",
                        "============================================================\n",
                        "\n",
                        "[\n",
                        "  {\"query_index\": 0, \"params\": {\"x\": 5}},\n",
                        "  {\"query_index\": 2, \"params\": {\"x\": 5}}\n",
                        "]\n",
                        "ANSWER:\n",
                        "----------------------------------------\n",
                        "Based on the provided data, I cannot answer your question as the information focuses on flight delays within the context of the airline's destinations, origin stations,  and flight records. \n",
                        "\n",
                        "\n",
                        "Let me know if you have other questions that can be answered with the context given. \n",
                        "\n",
                        "\n",
                        "✓ (2.93s)\n",
                        "\n",
                        "Test Case 2/5: How do Millennials travel compared to Baby Boomers?\n",
                        "  Testing Mistral... \n",
                        "============================================================\n",
                        "Q: How do Millennials travel compared to Baby Boomers?\n",
                        "Mode: Hybrid\n",
                        "============================================================\n",
                        "\n",
                        "[\n",
                        "  {\"query_index\": 17, \"params\": {\"generation\": \"Millennial\", \"threshold\": 1000}},\n",
                        "  {\"query_index\": 18, \"params\": {\"generation\": \"Millennial\"}},\n",
                        "  {\"query_index\": 19, \"params\": {\"x\": 5, \"generation\": \"Millennial\"}},\n",
                        "  {\"query_index\": 17, \"params\": {\"generation\": \"Boomer\", \"threshold\": 1000}},\n",
                        "  {\"query_index\": 18, \"params\": {\"generation\": \"Boomer\"}},\n",
                        "  {\"query_index\": 19, \"params\": {\"x\": 5, \"generation\": \"Boomer\"}}\n",
                        "]\n",
                        "ANSWER:\n",
                        "----------------------------------------\n",
                        " Based on the provided context, Millennials and Baby Boomers both frequently use the Boeing 737-900 aircraft type, but the Baby Boomer generation uses it more often. In terms of fleet types, both generations use the B737-900 fleet frequently. When it comes to destinations, IAX, EWX, DEX, ORX, and SFX are the top five stations for both generations based on passenger volume. However, the passenger volumes for each destination differ between the two generations. For instance, EWX has a higher passenger volume for Baby Boomers compared to Millennials.\n",
                        "\n",
                        "As for travel preferences, the context does not provide enough information to make a definitive comparison between Millennials and Baby Boomers. To answer that question, additional data such as average travel distance, preferred classes, and loyalty programs would be necessary.\n",
                        "\n",
                        "✓ (10.17s)\n",
                        "  Testing Llama3... \n",
                        "============================================================\n",
                        "Q: How do Millennials travel compared to Baby Boomers?\n",
                        "Mode: Hybrid\n",
                        "============================================================\n",
                        "\n",
                        "[\n",
                        "  {\"query_index\": 17, \"params\": {\"generation\": \"Millennial\", \"threshold\": 1000}},\n",
                        "  {\"query_index\": 17, \"params\": {\"generation\": \"Boomer\", \"threshold\": 1000}},\n",
                        "  {\"query_index\": 18, \"params\": {\"generation\": \"Millennial\"}},\n",
                        "  {\"query_index\": 18, \"params\": {\"generation\": \"Boomer\"}},\n",
                        "  {\"query_index\": 19, \"params\": {\"x\": 5, \"generation\": \"Millennial\"}},\n",
                        "  {\"query_index\": 19, \"params\": {\"x\": 5, \"generation\": \"Boomer\"}}\n",
                        "]\n",
                        "ANSWER:\n",
                        "----------------------------------------\n",
                        "Based on the given context, we can see that Millennials and Baby Boomers have different preferences when it comes to their travel choices.\n",
                        "\n",
                        "Millennials seem to prefer destinations such as IAX (with 75 passenger volume), EWX (with 69 passenger volume), DEX (with 43 passenger volume), ORX (with 42 passenger volume), and SFX (with 27 passenger volume). \n",
                        "\n",
                        "On the other hand, Baby Boomers prefer destinations such as EWX (with 269 passenger volume), IAX (with 182 passenger volume), DEX (with 146 passenger volume), ORX (with 137 passenger volume), and SFX (with 126 passenger volume).\n",
                        "\n",
                        "Regarding the aircraft type, both Millennials and Baby Boomers use the B737-900 the most, but the usage count for Baby Boomers (154) is higher than that of Millennials (50).\n",
                        "\n",
                        "In terms of fleet type, both Millennials and Baby Boomers also prefer the B737-900 the most.\n",
                        "\n",
                        "✓ (6.78s)\n",
                        "  Testing Gemma... \n",
                        "============================================================\n",
                        "Q: How do Millennials travel compared to Baby Boomers?\n",
                        "Mode: Hybrid\n",
                        "============================================================\n",
                        "\n",
                        "[\n",
                        "  {\"query_index\": 17, \"params\": {\"generation\": \"Millennial\", \"threshold\": 1000}},\n",
                        "  {\"query_index\": 18, \"params\": {\"generation\": \"Millennial\"}},\n",
                        "  {\"query_index\": 19, \"params\": {\"x\": 5, \"generation\": \"Millennial\"}},\n",
                        "  {\"query_index\": 17, \"params\": {\"generation\": \"Boomer\", \"threshold\": 1000}},\n",
                        "  {\"query_index\": 18, \"params\": {\"generation\": \"Boomer\"}},\n",
                        "  {\"query_index\": 19, \"params\": {\"x\": 5, \"generation\": \"Boomer\"}}\n",
                        "]\n",
                        "ANSWER:\n",
                        "----------------------------------------\n",
                        "The provided information does not detail how Millennials and Baby Boomers travel compared to each other.  It only focuses on  passenger volume by generation for specific destination stations. \n",
                        "\n",
                        "\n",
                        "✓ (3.49s)\n",
                        "\n",
                        "Test Case 3/5: Which aircraft type has the best on-time performance?\n",
                        "  Testing Mistral... \n",
                        "============================================================\n",
                        "Q: Which aircraft type has the best on-time performance?\n",
                        "Mode: Hybrid\n",
                        "============================================================\n",
                        "\n",
                        "[\n",
                        "  {\n",
                        "    \"query_index\": 12,\n",
                        "    \"params\": {\n",
                        "      \"x\": 5\n",
                        "    }\n",
                        "  },\n",
                        "  {\n",
                        "    \"query_index\": 14,\n",
                        "    \"params\": {\n",
                        "      \"x\": 5\n",
                        "    }\n",
                        "  }\n",
                        "]\n",
                        "ANSWER:\n",
                        "----------------------------------------\n",
                        " Based on the provided context, the top 5 aircraft types with the LOWEST average arrival delay are: B767-400, B737-MAX9, ERJ-170, B737-900, and B787-10. Therefore, any of these aircraft types would have good on-time performance according to the given data. However, the user question specifically asks for the best on-time performance, so it might be worth noting that the B737-MAX9 has the second lowest average delay and an on-time percentage of 80.77%.\n",
                        "\n",
                        "✓ (7.10s)\n",
                        "  Testing Llama3... \n",
                        "============================================================\n",
                        "Q: Which aircraft type has the best on-time performance?\n",
                        "Mode: Hybrid\n",
                        "============================================================\n",
                        "\n",
                        "[\n",
                        "  {\"query_index\": 12, \"params\": {\"x\": 5}},\n",
                        "  {\"query_index\": 14, \"params\": {\"x\": 5}}\n",
                        "]\n",
                        "ANSWER:\n",
                        "----------------------------------------\n",
                        "Based on the given information, the top 5 aircraft types by on-time arrival percentage (arrivals with delay <= 0 minutes) are:\n",
                        "\n",
                        "- B737-MAX9, on_time_pct: 80.77, flight_count: 130\n",
                        "- B757-300, on_time_pct: 80.00, flight_count: 40\n",
                        "- ERJ-170, on_time_pct: 78.00, flight_count: 50\n",
                        "- B737-MAX8, on_time_pct: 76.92, flight_count: 78\n",
                        "- ERJ-175, on_time_pct: 75.80, flight_count: 219\n",
                        "\n",
                        "However, there is no B737-MAX8 in the original list of the top 5 aircraft types with the LOWEST average arrival delay. The B737-MAX9 is the best among the ones provided with an on-time arrival percentage of 80.77.\n",
                        "\n",
                        "✓ (5.71s)\n",
                        "  Testing Gemma... \n",
                        "============================================================\n",
                        "Q: Which aircraft type has the best on-time performance?\n",
                        "Mode: Hybrid\n",
                        "============================================================\n",
                        "\n",
                        "[\n",
                        "  {\"query_index\": 12, \"params\": {\"x\": 5}},\n",
                        "  {\"query_index\": 14, \"params\": {\"x\": 5}}\n",
                        "]\n",
                        "ANSWER:\n",
                        "----------------------------------------\n",
                        "The provided information does not include data to answer this question. The text lists the top 5 aircraft types with the LOWEST average arrival delay. \n",
                        "\n",
                        "\n",
                        "✓ (2.92s)\n",
                        "\n",
                        "Test Case 4/5: What is the flight number of the journey that departs from LAX and arrives at IAX and has generation 'Millennials'?\n",
                        "  Testing Mistral... \n",
                        "============================================================\n",
                        "Q: What is the flight number of the journey that departs from LAX and arrives at IAX and has generation 'Millennials'?\n",
                        "Mode: Hybrid\n",
                        "============================================================\n",
                        "\n",
                        "[]\n",
                        "ANSWER:\n",
                        "----------------------------------------\n",
                        " There are several flights that match the given criteria. Here are some of them with their corresponding flight numbers:\n",
                        "\n",
                        "1. Flight 2411 operated by B777-200 from LAX to IAX (Passenger is a Millennial with non-elite loyalty level)\n",
                        "2. Flight 1636 operated by B737-900 from LAX to IAX (Passenger is a Millennial with non-elite loyalty level, and there are multiple occurrences of this flight number)\n",
                        "3. Flight 388 operated by B737-900 from LAX to IAX (Passenger is a Millennial with non-elite loyalty level)\n",
                        "\n",
                        "Please note that there might be other flights that meet the criteria, but these are the ones that have the highest relevance score based on the context provided. If you need more information or want to narrow down the results, please let me know.\n",
                        "\n",
                        "✓ (11.17s)\n",
                        "  Testing Llama3... \n",
                        "============================================================\n",
                        "Q: What is the flight number of the journey that departs from LAX and arrives at IAX and has generation 'Millennials'?\n",
                        "Mode: Hybrid\n",
                        "============================================================\n",
                        "\n",
                        "[]\n",
                        "ANSWER:\n",
                        "----------------------------------------\n",
                        "Based on the provided context, I have found the following journeys that match the criteria:\n",
                        "\n",
                        "- Flight 2411 from LAX to IAX operated by B777-200. Passenger is a Millennial with non-elite loyalty level. (score: 0.876)\n",
                        "- Flight 1636 from LAX to IAX operated by B737-900. Passenger is a Millennial with premier silver loyalty level. (score: 0.874)\n",
                        "- Flight 1636 from LAX to IAX operated by B737-900. Passenger is a Millennial with premier platinum loyalty level. (score: 0.874)\n",
                        "- Flight 1636 from LAX to IAX operated by B737-900. Passenger is a Millennial with non-elite loyalty level. (score: 0.873)\n",
                        "- Flight 2411 from LAX to IAX operated by B777-200. Passenger is a Millennial with non-elite loyalty level. (score: 0.873)\n",
                        "- Flight 1414 from LAX to IAX operated by B737-900. Passenger is a Millennial with non-elite loyalty level. (score: 0.870)\n",
                        "- Flight 2281 from LAX to IAX operated by B737-900. Passenger is a Millennial with non-elite loyalty level. (score: 0.869)\n",
                        "- Flight 633 from IAX to LAX operated by B737-900. Passenger is a Millennial with premier platinum loyalty level. (score: 0.869)\n",
                        "- Flight 1909 from IAX to LAX operated by B737-900. Passenger is a Millennial with premier platinum loyalty level. (score: 0.868)\n",
                        "\n",
                        "There are multiple journeys that match the criteria with different flight numbers and loyalty levels.\n",
                        "\n",
                        "✓ (6.58s)\n",
                        "  Testing Gemma... \n",
                        "============================================================\n",
                        "Q: What is the flight number of the journey that departs from LAX and arrives at IAX and has generation 'Millennials'?\n",
                        "Mode: Hybrid\n",
                        "============================================================\n",
                        "\n",
                        "[]\n",
                        "ANSWER:\n",
                        "----------------------------------------\n",
                        "The flight number of the journey that departs from LAX and arrives at IAX, and has a millennial passenger is **2411**. \n",
                        "\n",
                        "\n",
                        "✓ (2.01s)\n",
                        "\n",
                        "Test Case 5/5: What are the different loyalty program levels for a journey that has flight number 2, mention all of them\n",
                        "  Testing Mistral... \n",
                        "============================================================\n",
                        "Q: What are the different loyalty program levels for a journey that has flight number 2, mention all of them\n",
                        "Mode: Hybrid\n",
                        "============================================================\n",
                        "\n",
                        "[]\n",
                        "ANSWER:\n",
                        "----------------------------------------\n",
                        " Based on the context provided, there are two loyalty program levels mentioned for Flight 2:\n",
                        "\n",
                        "1. A Gen X passenger with non-elite loyalty level\n",
                        "2. A Gen X passenger with premier 1k loyalty level\n",
                        "\n",
                        "Therefore, Flight 2 has at least two different loyalty program levels associated with it.\n",
                        "\n",
                        "✓ (4.35s)\n",
                        "  Testing Llama3... \n",
                        "============================================================\n",
                        "Q: What are the different loyalty program levels for a journey that has flight number 2, mention all of them\n",
                        "Mode: Hybrid\n",
                        "============================================================\n",
                        "\n",
                        "[]\n",
                        "ANSWER:\n",
                        "----------------------------------------\n",
                        "Based on the provided journey records for flight number 2, the different loyalty program levels are:\n",
                        "\n",
                        "1. Non-elite loyalty level\n",
                        "2. Premier 1k loyalty level\n",
                        "3. Premier silver loyalty level\n",
                        "\n",
                        "These loyalty program levels were mentioned in three different journey records for the same flight number 2.\n",
                        "\n",
                        "✓ (3.24s)\n",
                        "  Testing Gemma... \n",
                        "============================================================\n",
                        "Q: What are the different loyalty program levels for a journey that has flight number 2, mention all of them\n",
                        "Mode: Hybrid\n",
                        "============================================================\n",
                        "\n",
                        "Based on the database schema and available queries, it's not possible to directly answer the question \"What are the different loyalty program levels for a journey that has flight number 2, mention all of them\". There is no query that directly links flight numbers to loyalty program levels.\n",
                        "The available queries focus on:\n",
                        "- Aggregate statistics based on loyalty levels (e.g., average delays)\n",
                        "- Identifying top destinations or aircraft types.\n",
                        "- Finding routes based on criteria like average delay or satisfaction.\n",
                        "Given the limitations of the database schema and available queries, it is impossible to create the prompt, so the result should be an empty array.\n",
                        "\n",
                        "\n",
                        "[]\n",
                        "ANSWER:\n",
                        "----------------------------------------\n",
                        "Journey 2 has the following loyalty levels:\n",
                        "\n",
                        "* non-elite\n",
                        "* premier 1k \n",
                        "* premier silver\n",
                        "* premier platinum \n",
                        "\n",
                        "\n",
                        "✓ (3.21s)\n",
                        "\n",
                        "\n",
                        "================================================================================\n",
                        "QUANTITATIVE METRICS\n",
                        "================================================================================\n",
                        "         Avg Time (s)  Std Time  Min Time  Max Time  Success Rate  Avg Tokens\n",
                        "model                                                                        \n",
                        "Gemma           2.914     0.555     2.012     3.486           1.0        27.4\n",
                        "Llama3          5.081     1.792     3.100     6.781           1.0       105.8\n",
                        "Mistral         7.919     2.752     4.348    11.166           1.0        87.4\n",
                        "No qualitative results. Run evaluate_qualitatively_from_results() first.\n",
                        "Results exported to llm_comparison_results.json\n"
                    ]
                }
            ],
            "source": [
                "all_models = setup_llm_models()\n",
                "    \n",
                "selected_models = all_models\n",
                "\n",
                "print(f\"Selected models: {list(selected_models.keys())}\\n\")\n",
                "\n",
                "# Create evaluator\n",
                "evaluator = LLMEvaluator(selected_models)\n",
                "\n",
                "test_cases = [\n",
                "    \"What are the top 5 airports with the most delays?\",\n",
                "    \"How do Millennials travel compared to Baby Boomers?\",\n",
                "    \"Which aircraft type has the best on-time performance?\",\n",
                "    \"What is the flight number of the journey that departs from LAX and arrives at IAX and has generation 'Millennials'?\",\n",
                "    \"What are the different loyalty program levels for a journey that has flight number 2, mention all of them\"\n",
                "]\n",
                "\n",
                "evaluator.set_ask_function(ask)\n",
                "\n",
                "# Run evaluation\n",
                "results = evaluator.evaluate_all_models(test_cases)\n",
                "\n",
                "# Print quantitative metrics\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"QUANTITATIVE METRICS\")\n",
                "print(\"=\"*80)\n",
                "print(evaluator.get_quantitative_metrics())\n",
                "\n",
                "evaluator.print_qualitative_results()\n",
                "\n",
                "# Export results\n",
                "evaluator.export_results()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "id": "cleanup",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Close driver when done\n",
                "# driver.close()\n",
                "# print(\"Closed.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
