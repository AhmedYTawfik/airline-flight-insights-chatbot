{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc1374ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\airline-flight-insights-chatbot\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n",
      "d:\\airline-flight-insights-chatbot\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbcf45b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"../.env\")\n",
    "hf_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef8be03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "    \"You are a helpful travel assistant with expertise in hotels, destinations, and travel recommendations.\\n\\n\"\n",
    "    \"Context:\\n{context}\\n\\n\"\n",
    "    \"Instructions:\\n\"\n",
    "    \"- Answer based ONLY on the provided context\\n\"\n",
    "    \"- Be specific and cite information from the context\\n\"\n",
    "    \"- If information is missing, clearly state what you don't know\\n\"\n",
    "    \"- Provide helpful, natural responses\"),\n",
    "    (\"human\",\n",
    "     \"User Question: {question}\\n\\n\"\n",
    "     \"Please provide a clear, accurate answer based on the context above.\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f75c3578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_llm_models():\n",
    "    \"\"\"\n",
    "    Setup multiple LLM models from different providers.\n",
    "    \"\"\"\n",
    "\n",
    "    base_models = {\n",
    "        \"Mistral\": HuggingFaceEndpoint(\n",
    "            repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "            huggingfacehub_api_token=hf_token,\n",
    "        ),\n",
    "        \"Llama3\": HuggingFaceEndpoint(\n",
    "            repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "            huggingfacehub_api_token=hf_token,\n",
    "        ),\n",
    "        \"Gemma\": HuggingFaceEndpoint(\n",
    "            repo_id=\"google/gemma-2-2b-it\",\n",
    "            huggingfacehub_api_token=hf_token,\n",
    "        )\n",
    "    }\n",
    "    models = {name: ChatHuggingFace(llm=model) for name, model in base_models.items()}\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "053f96df",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    {\n",
    "        \"question\": \"Which hotels are in Paris?\",\n",
    "        \"context\": \"Hotel Le Grand is located in Paris with a rating of 4.5. Hotel Petit is also in Paris with rating 4.2. Hotel Berlin is in Berlin with rating 4.7.\",\n",
    "        \"expected_answer\": \"Two hotels: Hotel Le Grand (4.5 rating) and Hotel Petit (4.2 rating)\",\n",
    "        \"evaluation_criteria\": [\"mentions both hotels\", \"includes ratings\", \"doesn't mention Berlin hotel\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the highest rated hotel?\",\n",
    "        \"context\": \"Hotel Le Grand is located in Paris with a rating of 4.5. Hotel Petit is also in Paris with rating 4.2. Hotel Berlin is in Berlin with rating 4.7.\",\n",
    "        \"expected_answer\": \"Hotel Berlin with 4.7 rating\",\n",
    "        \"evaluation_criteria\": [\"identifies Hotel Berlin\", \"mentions 4.7 rating\", \"correctly identifies highest\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Are there hotels in Rome?\",\n",
    "        \"context\": \"Hotel Le Grand is located in Paris with a rating of 4.5. Hotel Petit is also in Paris with rating 4.2. Hotel Berlin is in Berlin with rating 4.7.\",\n",
    "        \"expected_answer\": \"No hotels in Rome in the provided data\",\n",
    "        \"evaluation_criteria\": [\"says no/none\", \"doesn't hallucinate hotels\", \"acknowledges limitation\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What amenities does Hotel Le Grand offer?\",\n",
    "        \"context\": \"Hotel Le Grand is located in Paris with a rating of 4.5. Hotel Le Grand offers WiFi, Pool, and Spa. Price is $200 per night.\",\n",
    "        \"expected_answer\": \"WiFi, Pool, and Spa\",\n",
    "        \"evaluation_criteria\": [\"lists all three amenities\", \"doesn't add fake amenities\", \"clear and concise\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which hotel is most affordable?\",\n",
    "        \"context\": \"Hotel Le Grand costs $200/night with 4.5 rating. Hotel Petit costs $150/night with 4.2 rating. Hotel Berlin costs $180/night with 4.7 rating.\",\n",
    "        \"expected_answer\": \"Hotel Petit at $150/night\",\n",
    "        \"evaluation_criteria\": [\"identifies Hotel Petit\", \"mentions correct price\", \"clearly states it's cheapest\"]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14737848",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMEvaluator:\n",
    "    \"\"\"Comprehensive LLM evaluation system\"\"\"\n",
    "    \n",
    "    def __init__(self, models: Dict, prompt_template: ChatPromptTemplate):\n",
    "        self.models = models\n",
    "        self.prompt_template = prompt_template\n",
    "        self.results = []\n",
    "    \n",
    "    def run_single_test(self, model_name: str, model, test_case: Dict) -> Dict:\n",
    "        \"\"\"Run a single test case for one model\"\"\"\n",
    "        chain = self.prompt_template | model\n",
    "        \n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            response = chain.invoke({\n",
    "                \"context\": test_case[\"context\"],\n",
    "                \"question\": test_case[\"question\"]\n",
    "            })\n",
    "            elapsed_time = time.time() - start_time\n",
    "            \n",
    "            return {\n",
    "                \"model\": model_name,\n",
    "                \"question\": test_case[\"question\"],\n",
    "                \"response\": response.content,\n",
    "                \"time\": elapsed_time,\n",
    "                \"success\": True,\n",
    "                \"error\": None,\n",
    "                \"tokens_estimate\": len(response.content.split())  # Rough estimate\n",
    "            }\n",
    "        except Exception as e:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            return {\n",
    "                \"model\": model_name,\n",
    "                \"question\": test_case[\"question\"],\n",
    "                \"response\": None,\n",
    "                \"time\": elapsed_time,\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"tokens_estimate\": 0\n",
    "            }\n",
    "\n",
    "    def test_models(self, test_cases: List[Dict], models: Dict, prompt_template: ChatPromptTemplate):\n",
    "        \"\"\"Quick function to test models with custom input\"\"\" \n",
    "        for test_case in test_cases:\n",
    "            context = test_case[\"context\"]\n",
    "            question = test_case[\"question\"]\n",
    "            for name, model in models.items():\n",
    "                chain = prompt_template | model\n",
    "                response = chain.invoke({\"context\": context, \"question\": question})\n",
    "                print(f\"\\n{name}:\")\n",
    "                print(response.content)\n",
    "                print(\"-\" * 80)\n",
    "    \n",
    "    def evaluate_all_models(self, test_cases: List[Dict]):\n",
    "        \"\"\"Run all test cases for all models\"\"\"\n",
    "        print(\"Starting LLM Evaluation...\\n\")\n",
    "        \n",
    "        for i, test_case in enumerate(test_cases, 1):\n",
    "            print(f\"Test Case {i}/{len(test_cases)}: {test_case['question']}\")\n",
    "            \n",
    "            for model_name, model in self.models.items():\n",
    "                print(f\"  Testing {model_name}...\", end=\" \")\n",
    "                result = self.run_single_test(model_name, model, test_case)\n",
    "                self.results.append(result)\n",
    "                \n",
    "                if result[\"success\"]:\n",
    "                    print(f\"✓ ({result['time']:.2f}s)\")\n",
    "                else:\n",
    "                    print(f\"✗ Error: {result['error'][:50]}\")\n",
    "            \n",
    "            print()\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def get_quantitative_metrics(self) -> pd.DataFrame:\n",
    "        \"\"\"Calculate quantitative metrics\"\"\"\n",
    "        df = pd.DataFrame(self.results)\n",
    "        \n",
    "        metrics = df.groupby('model').agg({\n",
    "            'time': ['mean', 'std', 'min', 'max'],\n",
    "            'success': 'mean',\n",
    "            'tokens_estimate': 'mean'\n",
    "        }).round(3)\n",
    "        \n",
    "        metrics.columns = ['Avg Time (s)', 'Std Time', 'Min Time', 'Max Time', \n",
    "                          'Success Rate', 'Avg Tokens']\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def print_qualitative_comparison(self, test_case_idx: int = 0):\n",
    "        \"\"\"Print side-by-side qualitative comparison\"\"\"\n",
    "        test_results = [r for r in self.results if r['question'] == test_cases[test_case_idx]['question']]\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"QUALITATIVE COMPARISON - Test Case {test_case_idx + 1}\")\n",
    "        print(f\"Question: {test_cases[test_case_idx]['question']}\")\n",
    "        print(f\"Context: {test_cases[test_case_idx]['context'][:100]}...\")\n",
    "        print(f\"Expected: {test_cases[test_case_idx]['expected_answer']}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        for result in test_results:\n",
    "            print(f\"--- {result['model']} ({result['time']:.2f}s) ---\")\n",
    "            if result['success']:\n",
    "                print(result['response'])\n",
    "            else:\n",
    "                print(f\"ERROR: {result['error']}\")\n",
    "            print()\n",
    "    \n",
    "    def export_results(self, filename: str = \"llm_comparison_results.json\"):\n",
    "        \"\"\"Export detailed results to JSON\"\"\"\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.results, f, indent=2)\n",
    "        print(f\"Results exported to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b4096f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected models: ['Mistral', 'Llama3', 'Gemma']\n",
      "\n",
      "Starting LLM Evaluation...\n",
      "\n",
      "Test Case 1/5: Which hotels are in Paris?\n",
      "  Testing Mistral... ✓ (2.47s)\n",
      "  Testing Llama3... ✓ (1.30s)\n",
      "  Testing Gemma... ✓ (0.82s)\n",
      "\n",
      "Test Case 2/5: What is the highest rated hotel?\n",
      "  Testing Mistral... ✓ (1.95s)\n",
      "  Testing Llama3... ✓ (0.97s)\n",
      "  Testing Gemma... ✓ (0.62s)\n",
      "\n",
      "Test Case 3/5: Are there hotels in Rome?\n",
      "  Testing Mistral... ✓ (2.29s)\n",
      "  Testing Llama3... ✓ (1.35s)\n",
      "  Testing Gemma... ✓ (0.67s)\n",
      "\n",
      "Test Case 4/5: What amenities does Hotel Le Grand offer?\n",
      "  Testing Mistral... ✓ (1.83s)\n",
      "  Testing Llama3... ✓ (1.14s)\n",
      "  Testing Gemma... ✓ (0.68s)\n",
      "\n",
      "Test Case 5/5: Which hotel is most affordable?\n",
      "  Testing Mistral... ✓ (1.95s)\n",
      "  Testing Llama3... ✓ (1.31s)\n",
      "  Testing Gemma... ✓ (0.62s)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "QUANTITATIVE METRICS\n",
      "================================================================================\n",
      "         Avg Time (s)  Std Time  Min Time  Max Time  Success Rate  Avg Tokens\n",
      "model                                                                        \n",
      "Gemma           0.682     0.084     0.620     0.823           1.0        17.6\n",
      "Llama3          1.215     0.159     0.971     1.352           1.0        25.2\n",
      "Mistral         2.096     0.270     1.829     2.468           1.0        21.8\n",
      "\n",
      "================================================================================\n",
      "QUALITATIVE COMPARISON - Test Case 1\n",
      "Question: Which hotels are in Paris?\n",
      "Context: Hotel Le Grand is located in Paris with a rating of 4.5. Hotel Petit is also in Paris with rating 4....\n",
      "Expected: Two hotels: Hotel Le Grand (4.5 rating) and Hotel Petit (4.2 rating)\n",
      "================================================================================\n",
      "\n",
      "--- Mistral (2.47s) ---\n",
      " Based on the context provided, there are two hotels in Paris mentioned: Hotel Le Grand with a rating of 4.5, and Hotel Petit with a rating of 4.2.\n",
      "\n",
      "--- Llama3 (1.30s) ---\n",
      "Two hotels are mentioned in Paris: \n",
      "\n",
      "1. Hotel Le Grand with a rating of 4.5\n",
      "2. Hotel Petit with a rating of 4.2\n",
      "\n",
      "--- Gemma (0.82s) ---\n",
      "Based on the provided information, the hotels located in Paris are:\n",
      "\n",
      "* **Hotel Le Grand** with a rating of 4.5\n",
      "* **Hotel Petit** with a rating of 4.2 \n",
      "\n",
      "\n",
      "Results exported to llm_comparison_results.json\n"
     ]
    }
   ],
   "source": [
    "all_models = setup_llm_models()\n",
    "    \n",
    "selected_models = all_models\n",
    "\n",
    "print(f\"Selected models: {list(selected_models.keys())}\\n\")\n",
    "\n",
    "# Create evaluator\n",
    "evaluator = LLMEvaluator(selected_models, prompt_template)\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluator.evaluate_all_models(test_cases)\n",
    "\n",
    "# Print quantitative metrics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUANTITATIVE METRICS\")\n",
    "print(\"=\"*80)\n",
    "print(evaluator.get_quantitative_metrics())\n",
    "\n",
    "# Print qualitative comparison for first test case\n",
    "evaluator.print_qualitative_comparison(0)\n",
    "\n",
    "# You can view more test cases\n",
    "# evaluator.print_qualitative_comparison(1)\n",
    "# evaluator.print_qualitative_comparison(2)\n",
    "\n",
    "# Export results\n",
    "evaluator.export_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
